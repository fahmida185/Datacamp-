{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will parse the date\n",
    "def get_day(x): return dt.datetime(x.year, x.month, x.day) \n",
    "\n",
    "# Create InvoiceDay column\n",
    "online['InvoiceDay'] = online['InvoiceDate'].apply(get_day) \n",
    "\n",
    "# Group by CustomerID and select the InvoiceDay value\n",
    "grouping = online.groupby('CustomerID')['InvoiceDay'] \n",
    "\n",
    "# Assign a minimum InvoiceDay value to the dataset\n",
    "online['CohortDay'] = grouping.transform('min')\n",
    "\n",
    "# View the top 5 rows\n",
    "print(online.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the integers for date parts from the InvoiceDaycolumn\n",
    "invoice_year, invoice_month, invoice_day = get_date_int(online, 'InvoiceDay')\n",
    "\n",
    "# Get the integers for date parts from the CohortDay column\n",
    "cohort_year, cohort_month, cohort_day = get_date_int(online, 'CohortDay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate difference in years\n",
    "years_diff = invoice_year - cohort_year\n",
    "\n",
    "# Calculate difference in months\n",
    "months_diff = invoice_month - cohort_month\n",
    "\n",
    "# Calculate difference in days\n",
    "days_diff = invoice_day - cohort_day\n",
    "\n",
    "# Extract the difference in days from all previous values\n",
    "online['CohortIndex'] = years_diff * 365 + months_diff * 30 + days_diff + 1\n",
    "print(online.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique values per customer ID\n",
    "cohort_data = grouping['CustomerID'].apply(pd.Series.nunique).reset_index()\n",
    "\n",
    "# Create a pivot \n",
    "cohort_counts = cohort_data.pivot(index='CohortMonth', columns='CohortIndex', values='CustomerID')\n",
    "\n",
    "# Select the first column and store it to cohort_sizes\n",
    "cohort_sizes = cohort_counts.iloc[:,0]\n",
    "\n",
    "# Divide the cohort count by cohort sizes along the rows\n",
    "retention = cohort_counts.divide(cohort_sizes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a groupby object and pass the monthly cohort and cohort index as a list\n",
    "grouping = online.groupby(['CohortMonth', 'CohortIndex']) \n",
    "\n",
    "# Calculate the average of the unit price \n",
    "cohort_data = grouping['UnitPrice'].mean()\n",
    "\n",
    "# Reset the index of cohort_data\n",
    "cohort_data = cohort_data.reset_index()\n",
    "\n",
    "# Create a pivot \n",
    "average_quantity = cohort_data.pivot(index='CohortMonth', columns='CohortIndex', values='UnitPrice')\n",
    "print(average_quantity.round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import seaborn package as sns\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize an 8 by 6 inches plot figure\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Add a title\n",
    "plt.title('Average Spend by Monthly Cohorts')\n",
    "\n",
    "# Create the heatmap\n",
    "sns.heatmap(average_quantity, annot=True, cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spend quartile with 4 groups - a range between 1 and 5\n",
    "spend_quartile = pd.qcut(data['Spend'], q=4, labels=range(1,5))\n",
    "\n",
    "# Assign the quartile values to the Spend_Quartile column in data\n",
    "data['Spend_Quartile'] = spend_quartile\n",
    "\n",
    "# Print data with sorted Spend values\n",
    "print(data.sort_values('Spend'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store labels from 4 to 1 in a decreasing order\n",
    "r_labels = list(range(4, 0, -1))\n",
    "\n",
    "# Create a spend quartile with 4 groups and pass the previously created labels \n",
    "recency_quartiles = pd.qcut(data['Recency_Days'], q=4, labels=r_labels)\n",
    "\n",
    "# Assign the quartile values to the Recency_Quartile column in `data`\n",
    "data['Recency_Quartile'] = recency_quartiles \n",
    "\n",
    "# Print `data` with sorted Recency_Days values\n",
    "print(data.sort_values('Recency_Days'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Recency, Frequency and Monetary value for each customer \n",
    "datamart = online.groupby(['CustomerID']).agg({\n",
    "    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,\n",
    "    'InvoiceNo': 'count',\n",
    "    'TotalSum': 'sum'})\n",
    "\n",
    "# Rename the columns \n",
    "datamart.rename(columns={'InvoiceDate': 'Recency',\n",
    "                         'InvoiceNo': 'Frequency',\n",
    "                         'TotalSum': 'MonetaryValue'}, inplace=True)\n",
    "\n",
    "# Print top 5 rows\n",
    "print(datamart.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for Recency and Frequency\n",
    "r_labels = range(3, 0, -1); f_labels = range(1, 4)\n",
    "\n",
    "# Assign these labels to three equal percentile groups \n",
    "r_groups = pd.qcut(datamart['Recency'], q=3, labels=r_labels)\n",
    "\n",
    "# Assign these labels to three equal percentile groups \n",
    "f_groups = pd.qcut(datamart['Frequency'], q=3, labels=f_labels)\n",
    "\n",
    "# Create new columns R and F\n",
    "datamart = datamart.assign(R=r_groups.values, F=f_groups.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for MonetaryValue \n",
    "m_labels = range(1, 4)\n",
    "\n",
    "# Assign these labels to three equal percentile groups\n",
    "m_groups = pd.qcut(datamart['MonetaryValue'], q=3, labels=m_labels)\n",
    "\n",
    "# Create new column M\n",
    "datamart = datamart.assign(M=m_groups.values)\n",
    "\n",
    "# Calculate RFM_Score\n",
    "datamart['RFM_Score'] = datamart[['R','F','M']].sum(axis=1)\n",
    "print(datamart['RFM_Score'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rfm_level function\n",
    "def rfm_level(df):\n",
    "    if df['RFM_Score'] >= 10:\n",
    "        return 'Top'\n",
    "    elif ((df['RFM_Score'] >= 6) and (df['RFM_Score'] < 10)):\n",
    "        return 'Middle'\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "# Create a new variable RFM_Level\n",
    "datamart['RFM_Level'] = datamart.apply(rfm_level, axis=1)\n",
    "\n",
    "# Print the header with the top 5 rows to the console.\n",
    "print(datamart.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average values for each RFM_Level, and return a size of each segment \n",
    "rfm_level_agg = datamart.groupby('RFM_Level').agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': 'mean',\n",
    "  \n",
    "  \t# Return the size of each segment\n",
    "    'MonetaryValue': ['mean', 'count']\n",
    "}).round(1)\n",
    "\n",
    "# Print the aggregated dataset\n",
    "print(rfm_level_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the average values of the variables in the dataset\n",
    "print(data.mean())\n",
    "\n",
    "# Print the standard deviation of the variables in the dataset\n",
    "print(data.std())\n",
    "\n",
    "# Use `describe` function to get key statistics of the dataset\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detect skewed variables\n",
    "# Plot distribution of var1\n",
    "plt.subplot(3, 1, 1); sns.distplot(data['var1'])\n",
    "\n",
    "# Plot distribution of var2\n",
    "plt.subplot(3, 1, 2); sns.distplot(data['var2'])\n",
    "\n",
    "# Plot distribution of var3\n",
    "plt.subplot(3, 1, 3); sns.distplot(data['var3'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manage Skewness\n",
    "# Apply log transformation to var2\n",
    "data['var2_log'] = np.log(data['var2'])\n",
    "\n",
    "# Apply log transformation to var3\n",
    "data['var3_log'] = np.log(data['var3'])\n",
    "\n",
    "# Create a subplot of the distribution of var2_log\n",
    "plt.subplot(2, 1, 1); sns.distplot(data['var2_log'])\n",
    "\n",
    "# Create a subplot of the distribution of var3_log\n",
    "plt.subplot(2, 1, 2); sns.distplot(data['var3_log'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Center and scale manually\n",
    "# Center the data by subtracting average values from each entry\n",
    "data_centered = data - data.mean()\n",
    "\n",
    "# Scale the data by dividing each entry by standard deviation\n",
    "data_scaled = data / data.std()\n",
    "\n",
    "# Normalize the data by applying both centering and scaling\n",
    "data_normalized = (data - data.mean()) / data.std()\n",
    "\n",
    "# Print summary statistics to make sure average is zero and standard deviation is one\n",
    "print(data_normalized.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Center and scale with StandardScaler()\n",
    "# Initialize a scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "scaler.fit(data)\n",
    "\n",
    "# Scale and center the data\n",
    "data_normalized = scaler.transform(data)\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "data_normalized = pd.DataFrame(data_normalized, index=data.index, columns=data.columns)\n",
    "\n",
    "# Print summary statistics\n",
    "print(data_normalized.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Preprocessing PipeLine\n",
    "#Visualizing The Distribution\n",
    "# Plot recency distribution\n",
    "plt.subplot(3, 1, 1); sns.distplot(datamart_rfm['Recency'])\n",
    "\n",
    "# Plot frequency distribution\n",
    "plt.subplot(3, 1, 2); sns.distplot(datamart_rfm['Frequency'])\n",
    "\n",
    "# Plot monetary value distribution\n",
    "plt.subplot(3, 1, 3); sns.distplot(datamart_rfm['MonetaryValue'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "#####Pre-process RFM data\n",
    "# Unskew the data\n",
    "datamart_log = np.log(datamart_rfm)\n",
    "\n",
    "# Initialize a standard scaler and fit it\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(datamart_log)\n",
    "\n",
    "# Scale and center the data\n",
    "datamart_normalized = scaler.transform(datamart_log)\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "datamart_normalized = pd.DataFrame(data=datamart_normalized, index=datamart_rfm.index, columns=datamart_rfm.columns)\n",
    "\n",
    "######Visualize the normalized variables\n",
    "# Plot recency distribution\n",
    "plt.subplot(3, 1, 1); sns.distplot(datamart_normalized['Recency'])\n",
    "\n",
    "# Plot frequency distribution\n",
    "plt.subplot(3, 1, 2); sns.distplot(datamart_normalized['Frequency'])\n",
    "\n",
    "# Plot monetary value distribution\n",
    "plt.subplot(3, 1, 3); sns.distplot(datamart_normalized['MonetaryValue'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Customer Segmentation Running KMeans\n",
    "# Import KMeans \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=1) \n",
    "\n",
    "# Fit k-means clustering on the normalized data set\n",
    "kmeans.fit(datamart_normalized)\n",
    "\n",
    "# Extract cluster labels\n",
    "cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######Assign labels to raw data\n",
    "# Create a DataFrame by adding a new cluster label column\n",
    "datamart_rfm_k3 = datamart_rfm.assign(Cluster=cluster_labels)\n",
    "\n",
    "# Group the data by cluster\n",
    "grouped = datamart_rfm_k3.groupby(['Cluster'])\n",
    "\n",
    "# Calculate average RFM values and segment sizes per cluster value\n",
    "grouped.agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': 'mean',\n",
    "    'MonetaryValue': ['mean', 'count']\n",
    "  }).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Calculate sum of squared errors\n",
    "# Fit KMeans and calculate SSE for each k\n",
    "for k in range(1, 21):\n",
    "  \n",
    "    # Initialize KMeans with k clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=1)\n",
    "    \n",
    "    # Fit KMeans on the normalized dataset\n",
    "    kmeans.fit(data_normalized)\n",
    "    \n",
    "    # Assign sum of squared distances to k element of dictionary\n",
    "    sse[k] = kmeans.inertia_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Plot sum of squared errors\n",
    "# Add the plot title \"The Elbow Method\"\n",
    "plt.title('The Elbow Method')\n",
    "\n",
    "# Add X-axis label \"k\"\n",
    "plt.xlabel('k')\n",
    "\n",
    "# Add Y-axis label \"SSE\"\n",
    "plt.ylabel('SSE')\n",
    "\n",
    "# Plot SSE values for each key in the dictionary\n",
    "sns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Prepare data for the snake plot\n",
    "# Melt the normalized dataset and reset the index\n",
    "datamart_melt = pd.melt(\n",
    "  \t\t\t\t\tdatamart_normalized.reset_index(), \n",
    "                        \n",
    "# Assign CustomerID and Cluster as ID variables                  \n",
    "                    id_vars=['CustomerID', 'Cluster'],\n",
    "\n",
    "# Assign RFM values as value variables\n",
    "                    value_vars=['Recency', 'Frequency', 'MonetaryValue'], \n",
    "                        \n",
    "# Name the variable and value\n",
    "                    var_name='Metric', value_name='Value'\n",
    "\t\t\t\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize snake plot\n",
    "# Add the plot title\n",
    "plt.title('Snake plot of normalized variables')\n",
    "\n",
    "# Add the x axis label\n",
    "plt.xlabel('Metric')\n",
    "\n",
    "# Add the y axis label\n",
    "plt.ylabel('Value')\n",
    "\n",
    "# Plot a line for each value of the cluster variable\n",
    "sns.lineplot(data=datamart_melt, x='Metric', y='Value', hue='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Calculate relative importance of each attribute\n",
    "# Calculate average RFM values for each cluster\n",
    "cluster_avg = datamart_rfm_k3.groupby(['Cluster']).mean() \n",
    "\n",
    "# Calculate average RFM values for the total customer population\n",
    "population_avg = datamart_rfm.mean()\n",
    "\n",
    "# Calculate relative importance of cluster's attribute value compared to population\n",
    "relative_imp = cluster_avg / population_avg - 1\n",
    "\n",
    "# Print relative importance score rounded to 2 decimals\n",
    "print(relative_imp.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######Plot relative importance heatmap\n",
    "# Initialize a plot with a figure size of 8 by 2 inches \n",
    "plt.figure(figsize=(8, 2))\n",
    "\n",
    "# Add the plot title\n",
    "plt.title('Relative importance of attributes')\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(data=relative_imp, annot=True, fmt='.2f', cmap='RdYlGn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### With 4 Inputs RFMT\n",
    "# Import StandardScaler \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Apply log transformation\n",
    "datamart_rfmt_log = np.log(datamart_rfmt)\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler(); scaler.fit(datamart_rfmt_log)\n",
    "\n",
    "# Transform and store the scaled data as datamart_rfmt_normalized\n",
    "datamart_rfmt_normalized = scaler.transform(datamart_rfmt_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Calculate and plot sum of squared errors\n",
    "# Fit KMeans and calculate SSE for each k\n",
    "for k in range(1, 11):\n",
    "  \n",
    "    # Initialize KMeans with k clusters and fit it \n",
    "    kmeans = KMeans(n_clusters=k, random_state=1).fit(datamart_rfmt_normalized)\n",
    "    \n",
    "    # Assign sum of squared distances to k element of the sse dictionary\n",
    "    sse[k] = kmeans.inertia_ \n",
    "\n",
    "# Add the plot title, x and y axis labels\n",
    "plt.title('The Elbow Method'); plt.xlabel('k'); plt.ylabel('SSE')\n",
    "\n",
    "# Plot SSE values for each k stored as keys in the dictionary\n",
    "sns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n",
    "plt.show()\n",
    "#The elbow is visible around 3-4 clusters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Build 4-cluster solution\n",
    "# Import KMeans \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize KMeans\n",
    "kmeans = KMeans(n_clusters=4, random_state=1) \n",
    "\n",
    "# Fit k-means clustering on the normalized data set\n",
    "kmeans.fit(datamart_rfmt_normalized)\n",
    "\n",
    "# Extract cluster labels\n",
    "cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########Analyze the segments\n",
    "# Create a new DataFrame by adding a cluster label column to datamart_rfmt\n",
    "datamart_rfmt_k4 = datamart_rfmt.assign(Cluster=cluster_labels)\n",
    "\n",
    "# Group by cluster\n",
    "grouped = datamart_rfmt_k4.groupby(['Cluster'])\n",
    "\n",
    "# Calculate average RFMT values and segment sizes for each cluster\n",
    "grouped.agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': 'mean',\n",
    "    'MonetaryValue': 'mean',\n",
    "    'Tenure': ['mean', 'count']\n",
    "  }).round(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
